{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMLS Mapping from survey to HPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Your CSV file path\n",
    "csv_file_path = \"test.csv\"\n",
    "# Read CSV, skip the first row, and select only the desired columns\n",
    "df = pd.read_csv(csv_file_path, skiprows=1, usecols=['question', 'question_HPO_code', 'question_HPO_label'])\n",
    "# Prepare empty columns for results\n",
    "df['hpo_codes'] = None\n",
    "df['confidence'] = None\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a data to test before officially starting the running\n",
    "from src.graph.builder import umls_mapping_graph\n",
    "result = umls_mapping_graph.invoke({'text': 'Atlanto-axial instability (neck spine is weak or unstable)'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary lab\n",
    "from src.graph.builder import umls_mapping_graph\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from src.config.agents import AGENT_LLM_MAP\n",
    "from src.prompts.template import apply_prompt_template\n",
    "\n",
    "\n",
    "API_BASE_URL = \"http://52.43.228.165:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the code\n",
    "for idx, row in df.iterrows():\n",
    "    question_text = row['question']\n",
    "\n",
    "    try:\n",
    "        result = umls_mapping_graph.invoke({\n",
    "            \"text\": question_text,\n",
    "            \"retries\": 0  \n",
    "        })\n",
    "\n",
    "        validated_mappings = result.get(\"validated_mappings\", [])\n",
    "\n",
    "        # set none, these three output is our goals\n",
    "        best_match_code = \"\"\n",
    "        best_match_term = \"\"\n",
    "        confidence = \"\"\n",
    "\n",
    "        if validated_mappings and isinstance(validated_mappings, list) and validated_mappings[0]:\n",
    "            best_mapping = validated_mappings[0]\n",
    "            best_match_code = best_mapping.get(\"best_match_code\", \"\")\n",
    "            best_match_term = best_mapping.get(\"best_match_term\", \"\")\n",
    "            confidence = best_mapping.get(\"confidence\", \"\")\n",
    "\n",
    "        df.at[idx, 'best_match_code'] = best_match_code\n",
    "        df.at[idx, 'best_match_term'] = best_match_term\n",
    "        df.at[idx, 'confidence'] = confidence\n",
    "\n",
    "    except Exception as e:\n",
    "        # Defensive skipping on error to prevent the entire batch from being interrupted\n",
    "        print(f\"❌ Error processing row {idx + 1}: {e}\")\n",
    "        df.at[idx, 'best_match_code'] = \"\"\n",
    "        df.at[idx, 'best_match_term'] = \"\"\n",
    "        df.at[idx, 'confidence'] = \"\"\n",
    "\n",
    "    print(f\"✅ Processed row {idx + 1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the result and check it \n",
    "df.to_csv(\"output_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get more data for model performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Get CUI code data through ‘term’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using extracted terms to search CUI\n",
    "def search_cui(term):\n",
    "    \"\"\" Search for CUIs using the API. \"\"\"\n",
    "    response = requests.get(f\"{API_BASE_URL}/cuis\", params={\"query\": term})\n",
    "    return response.json()\n",
    "\n",
    "# Defined CUI extraction function and extracts the first CUI returned\n",
    "def extract_first_cui_from_term(term):\n",
    "    \"\"\"\n",
    "    Given a term, query API and extract first CUI.\n",
    "    If term is None or empty, return None directly.\n",
    "    \"\"\"\n",
    "    if pd.isna(term) or str(term).strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        response = search_cui(term)\n",
    "        cuis = response.get('cuis', [])\n",
    "        if cuis:\n",
    "            return cuis[0]['cui']\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting CUI for term '{term}': {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['True CUI'] = None\n",
    "df['Predicted CUI'] = None\n",
    "# Run the code\n",
    "for idx, row in df.iterrows():\n",
    "    hpo_label = row['question_HPO_label']\n",
    "    hpo_code = row['question_HPO_code']\n",
    "    best_match_term = row['best_match_term']\n",
    "    best_match_code = row['best_match_code']\n",
    "    \n",
    "    # Get True CUI \n",
    "    true_cui = extract_first_cui_from_term(hpo_label)\n",
    "    df.at[idx, 'True CUI'] = true_cui\n",
    "\n",
    "    # Get Predicted_CUI \n",
    "    if hpo_code == best_match_code:\n",
    "        # If hpo_code = predicted hpo_code，then True_CUI = Predicted CUI\n",
    "        df.at[idx, 'Predicted CUI'] = true_cui\n",
    "    else:\n",
    "        pred_cui = extract_first_cui_from_term(best_match_term)\n",
    "        df.at[idx, 'Predicted CUI'] = pred_cui\n",
    "\n",
    "# Check the results\n",
    "print(df[['question_HPO_label', 'True CUI', 'Predicted CUI']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get wu_palmer_similarity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def is_null(value):\n",
    "    return value is None or pd.isna(value) or str(value).strip() == ''\n",
    "\n",
    "def compute_wu_palmer_similarity(actual_output, expected_output):\n",
    "    \"\"\"Computes Wu-Palmer similarity between predicted and true CUI.\"\"\"\n",
    "    if is_null(actual_output) or is_null(expected_output):\n",
    "        return None\n",
    "    \n",
    "    if actual_output == expected_output:\n",
    "        return 1.0\n",
    "\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE_URL}/cuis/{actual_output}/{expected_output}/similarity/wu-palmer\")\n",
    "        logging.info(f\"Wu-Palmer API response: {response.status_code} - {response.text}\")\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            similarity = round(data.get(\"similarity\", 0), 4)\n",
    "            return similarity\n",
    "        else:\n",
    "            logging.error(f\"API returned non-200: {response.status_code}\")\n",
    "            return 0\n",
    "\n",
    "    except requests.Timeout:\n",
    "        logging.error(\"Timeout error while calling Wu-Palmer API\")\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error computing Wu-Palmer similarity: {e}\")\n",
    "        return 0\n",
    "\n",
    "# 先确认列名：\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# 批量计算 Wu-Palmer 相似度\n",
    "df['Wu-Palmer Similarity'] = df.apply(\n",
    "    lambda row: compute_wu_palmer_similarity(row['True CUI'], row['Predicted CUI']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the LLM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calculate method\n",
    "def rank_evaluate_with_llm(predicted_label, true_label, predicted_code, true_code, agent_llm_map):\n",
    "    if is_null(predicted_code) or is_null(true_code):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        llm = agent_llm_map[\"rank_evaluate_with_llm\"]\n",
    "        prompt = apply_prompt_template(\n",
    "    \"rank_evaluate_with_llm\", \n",
    "    {\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_code\": predicted_code,\n",
    "        \"true_code\": true_code\n",
    "    }\n",
    ")\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        if hasattr(response, \"choices\"):\n",
    "            score_text = response.choices[0].message.content.strip()\n",
    "        elif hasattr(response, \"content\"):\n",
    "            score_text = response.content.strip()\n",
    "        else:\n",
    "            logging.error(\"LLM response structure not recognized.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            score = float(score_text)\n",
    "            if score < 1 or score > 10:\n",
    "                logging.warning(f\"LLM returned out-of-range score: {score_text}\")\n",
    "                return None\n",
    "            return score\n",
    "\n",
    "        except ValueError:\n",
    "            logging.error(f\"LLM returned invalid score: {score_text}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calling LLM agent for ranking evaluation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "evaluate_func = partial(rank_evaluate_with_llm, agent_llm_map=AGENT_LLM_MAP)\n",
    "\n",
    "# Filter out rows without none values\n",
    "filtered_df = df.dropna(subset=['Predicted CUI', 'True CUI'])\n",
    "\n",
    "# Apply the function\n",
    "df['LLM Score'] = filtered_df.apply(\n",
    "    lambda row: evaluate_func(\n",
    "        row['best_match_term'], \n",
    "        row['question_HPO_label'], \n",
    "        row['Predicted CUI'], \n",
    "        row['True CUI']\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Get specificity by using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate_specificity_with_llm function\n",
    "def evaluate_specificity_with_llm(predicted_label, true_label, predicted_code, true_code, agent_llm_map):\n",
    "    if is_null(predicted_code) or is_null(true_code):\n",
    "        return None\n",
    "\n",
    "    print(\"agent_llm_map keys: \", agent_llm_map.keys())\n",
    "\n",
    "    try:\n",
    "        llm = agent_llm_map[\"evaluate_specificity_with_llm\"]\n",
    "\n",
    "        prompt = apply_prompt_template(\n",
    "            \"evaluate_specificity_with_llm\",\n",
    "            {\n",
    "                \"predicted_label\": predicted_label,\n",
    "                \"true_label\": true_label,\n",
    "                \"predicted_code\": predicted_code,\n",
    "                \"true_code\": true_code\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        if hasattr(response, \"choices\"):\n",
    "            specificity_text = response.choices[0].message.content.strip().lower().strip('\"').strip(\"'\")\n",
    "        elif hasattr(response, \"content\"):\n",
    "            specificity_text = response.content.strip().lower().strip('\"').strip(\"'\")\n",
    "        else:\n",
    "            logging.error(\"LLM response structure not recognized.\")\n",
    "            return None\n",
    "\n",
    "        valid_outputs = [\"exact match\", \"too specific\", \"too general\", \"related but not a match\", \"incorrect\"]\n",
    "        if specificity_text not in valid_outputs:\n",
    "            logging.warning(f\"LLM returned unexpected category: {specificity_text}\")\n",
    "            return \"unknown\"\n",
    "\n",
    "        return specificity_text\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calling LLM agent for specificity evaluation: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "specificity_func = partial(evaluate_specificity_with_llm,agent_llm_map=AGENT_LLM_MAP)\n",
    "# Filter out rows without none values\n",
    "filtered_df = df.dropna(subset=['Predicted CUI', 'True CUI'])\n",
    "\n",
    "df['Specificity Category'] = None  \n",
    "# Apply the function\n",
    "df.loc[filtered_df.index, 'Specificity Category'] = filtered_df.apply(\n",
    "    lambda row: specificity_func(\n",
    "        row['best_match_term'],\n",
    "        row['question_HPO_label'],\n",
    "        row['Predicted CUI'],\n",
    "        row['True CUI']\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results\n",
    "df.to_csv(\"output_file_cui_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
